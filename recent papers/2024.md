### ðŸ“Œ**Recent papers (2024)**
- [Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions](https://arxiv.org/pdf/2404.07214)
- [The Revolution of Multimodal Large Language Models: A Survey](https://arxiv.org/pdf/2402.12451)
- [CompCap: Improving Multimodal Large Language Models with Composite Captions](https://arxiv.org/pdf/2412.05243)
- [Apollo: An Exploration of Video Understanding in Large Multimodal Models](https://arxiv.org/pdf/2412.10360)
- [RegionGPT: Towards Region Understanding Vision Language Model](https://arxiv.org/pdf/2403.02330)
- [Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models](https://arxiv.org/html/2403.03003v1)
- [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/pdf/2403.05530)
- [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/pdf/2403.09611)
- [InternVideo2: Scaling Foundation Models for Multimodal Video Understanding](https://arxiv.org/pdf/2403.15377)
- [MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding](https://arxiv.org/pdf/2404.05726)
- [Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models](https://arxiv.org/pdf/2404.13013)
- [What matters when building vision-language models?](https://arxiv.org/pdf/2405.02246)
- [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/pdf/2405.09818)
- [ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models](https://arxiv.org/pdf/2405.15738)
- [OLIVE: Object Level In-Context Visual Embeddings](https://arxiv.org/pdf/2406.00872)
- [4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities](https://arxiv.org/pdf/2406.09406)
- [VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding](https://arxiv.org/pdf/2406.09418)
- [Generative Visual Instruction Tuning](https://arxiv.org/pdf/2406.11262)
- [Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/pdf/2406.16860)
- [TokenPacker: Efficient Visual Projector for Multimodal LLM](https://arxiv.org/pdf/2407.02392)
- [Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://arxiv.org/pdf/2412.05271)
- [Safety of Multimodal Large Language Models on Images and Texts](https://arxiv.org/pdf/2402.00357)
- [Vision-Language Models for Vision Tasks: A Survey](https://arxiv.org/pdf/2304.00685)
- [Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey](https://arxiv.org/html/2412.18619v1)
- [VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework](https://arxiv.org/pdf/2403.09027)
- [Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding](https://arxiv.org/pdf/2312.00081)
- [Towards Better Vision-Inspired Vision-Language Models](https://openaccess.thecvf.com/content/CVPR2024/html/Cao_Towards_Better_Vision-Inspired_Vision-Language_Models_CVPR_2024_paper.html)
- [Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models](https://arxiv.org/pdf/2311.17091)
- [Better Language Models Exhibit Higher Visual Alignment](https://arxiv.org/pdf/2410.07173)
- [FiVL: A Framework for Improved Vision-Language Alignment through the Lens of Training, Evaluation and Explainability](https://arxiv.org/pdf/2412.14672)
- [MC-LLaVA: Multi-Concept Personalized Vision-Language Model](https://arxiv.org/pdf/2411.11706)
- [SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models](https://arxiv.org/pdf/2408.12114)
- [A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for Accelerating Large VLMs](https://arxiv.org/pdf/2412.03324)
- [VHELM: A Holistic Evaluation of Vision Language Models](https://arxiv.org/html/2410.07112v2)
- [What's in the Image? A Deep-Dive into the Vision of Vision Language Models](https://arxiv.org/pdf/2411.17491)
- [Vision language models are blind](https://openaccess.thecvf.com/content/ACCV2024/html/Rahmanzadehgervi_Vision_language_models_are_blind_ACCV_2024_paper.html)